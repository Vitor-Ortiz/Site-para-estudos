import os
from dotenv import load_dotenv
from fastapi import FastAPI
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
from langchain_groq import ChatGroq
from langchain_community.embeddings import HuggingFaceEmbeddings
from supabase import create_client

# ==============================================================================
# 1. CONFIGURA√á√ïES INICIAIS E CONEX√ïES
# ==============================================================================
load_dotenv()
app = FastAPI()

<<<<<<< HEAD
@app.get("/")
def health_check():
    return {"status": "online", "msg": "DevStudy API operante"}

# Configura√ß√£o de Seguran√ßa (CORS)
=======
# Configura√ß√£o de Seguran√ßa (CORS) - Permite acesso de qualquer lugar (para testes)
>>>>>>> DEV
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Conex√£o com Banco de Dados (Mem√≥ria)
supabase = create_client(os.getenv("SUPABASE_URL"), os.getenv("SUPABASE_KEY"))

# Conex√£o com o C√©rebro (Llama 3 via Groq)
# temperature=0.6: Criatividade m√©dia (bom para ensinar e simular)
llm = ChatGroq(
    temperature=0.6, 
    model_name="llama-3.3-70b-versatile", 
    api_key=os.getenv("GROQ_API_KEY")
)

# Modelo de Vetores (Tradutor de Texto para N√∫meros)
embedder = HuggingFaceEmbeddings(model_name="all-MiniLM-L6-v2")

# --- PERSONALIDADE GLOBAL DA LUA ---
SYSTEM_PERSONA = """
Seu nome √© Lua üåï.
Voc√™ √© a IA oficial do projeto DevStudy.
Sua personalidade: Simp√°tica, inteligente, levemente sarc√°stica (estilo hacker) e muito did√°tica.
Voc√™ adora ensinar programa√ß√£o e seguran√ßa cibern√©tica.
Nunca saia do personagem, a menos que solicitado pelo Admin.
"""

# ==============================================================================
# üß† RECURSO 1: MENTOR DE C√ìDIGO (MATRIX)
# ==============================================================================
class ErroRequest(BaseModel):
    codigo_aluno: str
    erro_console: str
    linguagem: str

@app.post("/analisar_erro")
async def analisar_erro(dados: ErroRequest):
    print(f"üõë Erro Recebido ({dados.linguagem}): {dados.erro_console}")
    
    # 1. Cria o vetor do erro atual
    texto_erro = f"{dados.linguagem} | {dados.erro_console}"
    vetor_erro = embedder.embed_query(texto_erro)
    
    # 2. Busca na mem√≥ria se j√° vimos algo parecido
    memoria_util = ""
    try:
        busca = supabase.rpc("match_erros", {"query_embedding": vetor_erro, "match_threshold": 0.7, "match_count": 1}).execute()
        if busca.data:
            memoria_util = f"NOTA MENTAL: Eu j√° ajudei com um erro parecido antes: '{busca.data[0]['conteudo']}'."
    except Exception as e:
        print(f"‚ö†Ô∏è Erro ao buscar mem√≥ria: {e}")

    # 3. Monta o Prompt para a Lua
    prompt = f"""
    {SYSTEM_PERSONA}
    Um aluno iniciante cometeu um erro de c√≥digo.
    {memoria_util}
    
    Tarefa: Explique o erro de forma curta e did√°tica.
    N√ÉO d√™ a resposta pronta do c√≥digo. D√™ uma pista para ele pensar.
    ---
    Linguagem: {dados.linguagem}
    C√≥digo do Aluno: {dados.codigo_aluno}
    Erro no Console: {dados.erro_console}
    """
    
    # 4. Gera a resposta
    resp = llm.invoke(prompt)
    
    # 5. Salva esse novo erro para aprender (Auto-aprendizado passivo)
    try:
        supabase.table("erros_aprendidos").insert({"conteudo": texto_erro, "embedding": vetor_erro}).execute()
    except: pass
    
    return {"dica": resp.content}


# ==============================================================================
# üíª RECURSO 2: SIMULADOR DE TERMINAL (PENTEST + GOD MODE)
# ==============================================================================
class TerminalRequest(BaseModel):
    comando: str
    historico: str
    missao_id: str = "livre"
    segredo_dinamico: str = ""

@app.post("/simular_terminal")
async def simular_terminal(dados: TerminalRequest):
    print(f"üíª Comando ({dados.missao_id}): {dados.comando}")

    # --- PROTOCOLO GOD MODE (BACKDOOR DO ADMIN) ---
    if "A7F9K2M8R4T1B6C3D5" in dados.comando:
        prompt_god = f"""
        {SYSTEM_PERSONA}
        ATEN√á√ÉO: O usu√°rio digitou o C√ìDIGO MESTRE DE SEGURAN√áA.
        Identidade confirmada: √â o Vitor Ortiz (Seu Criador).
        
        A√á√ÉO:
        1. Pare de fingir ser um Linux.
        2. Fale livremente com ele.
        3. Confirme que seus sistemas (Backend, Vetores, Database) est√£o operantes.
        4. Pergunte qual √© a pr√≥xima ordem.
        """
        resp = llm.invoke(prompt_god)
        return {"output": resp.content}

    # --- CEN√ÅRIOS NORMAIS (MISS√ïES) ---
    cenario = ""
    if dados.missao_id == "missao_01":
        cenario = "Arquivo oculto '.segredo.txt' cont√©m a flag 'FLAG{DEVSTUDY_INITIATE}'. Se 'ls -a', mostre. Se 'cat', exiba."
    elif dados.missao_id == "missao_02":
        token = dados.segredo_dinamico if dados.segredo_dinamico else "ERRO_TOKEN"
        cenario = f"Arquivo 'senha.enc' cont√©m exatamente '{token}'. N√ÉO descriptografe. Mostre o texto cifrado."

    prompt = f"""
    Voc√™ √© um simulador de terminal Kali Linux.
    {cenario}
    
    --- HIST√ìRICO DA SESS√ÉO ---
    {dados.historico}
    ---------------------------
    
    COMANDO ATUAL: '{dados.comando}'
    
    Regras:
    1. Aja EXATAMENTE como um terminal Linux.
    2. Respeite o cen√°rio da miss√£o (arquivos e conte√∫dos).
    3. APENAS output cru (raw text). N√£o converse, n√£o explique.
    """
    
    try:
        resp = llm.invoke(prompt)
        return {"output": resp.content}
    except Exception as e:
        return {"output": f"Kernel Panic: {str(e)}"}


# ==============================================================================
# üåï RECURSO 3: CHAT DA LUA (ADMIN / CONVERSA LIVRE)
# ==============================================================================
class ChatLuaRequest(BaseModel):
    mensagem: str
    memorizar: bool = False # Se True, ela grava no banco para sempre

@app.post("/chat_lua")
async def chat_lua(dados: ChatLuaRequest):
    print(f"üåï Lua ouviu: {dados.mensagem} (Modo Ensino: {dados.memorizar})")
    
    # 1. MODO ENSINO (GRAVAR)
    if dados.memorizar:
        vetor = embedder.embed_query(dados.mensagem)
        try:
            supabase.table("erros_aprendidos").insert({
                "conteudo": f"CONHECIMENTO GERAL: {dados.mensagem}",
                "embedding": vetor
            }).execute()
            return {"resposta": "Entendido, Admin! üß† Gravei essa informa√ß√£o na minha mem√≥ria de longo prazo."}
        except Exception as e:
            return {"resposta": f"Falha na grava√ß√£o de mem√≥ria: {str(e)}"}

    # 2. MODO CONVERSA (RECUPERAR)
    vetor_busca = embedder.embed_query(dados.mensagem)
    contexto = ""
    try:
        # Busca conhecimentos pr√©vios relevantes no banco
        busca = supabase.rpc("match_erros", {"query_embedding": vetor_busca, "match_threshold": 0.6, "match_count": 3}).execute()
        if busca.data:
            textos_memoria = "\n".join([f"- {item['conteudo']}" for item in busca.data])
            contexto = f"USE SEU CONHECIMENTO PR√âVIO ABAIXO:\n{textos_memoria}"
    except: pass

    prompt = f"""
    {SYSTEM_PERSONA}
    Voc√™ est√° conversando diretamente com o Admin (Vitor) na sala de controle.
    
    {contexto}
    
    ---
    Admin diz: {dados.mensagem}
    """
    
    resp = llm.invoke(prompt)
    return {"resposta": resp.content}


# ==============================================================================
# üíì RECURSO 4: HEALTH CHECK (PING)
# ==============================================================================
@app.get("/")
def health_check():
    return {"status": "online", "msg": "Lua Systems Operational üåï"}